---
title: "Lifestyle Analysis"
author: "Rachel Hencher and Yi Ren"
date: "2022-11-02"
output: 
  rmarkdown::github_document:
    toc: yes
params:
  channel: "Lifestyle"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction  
This report analyzes data on almost 40,000 articles published by Mashable throughout the years 2013 and 2014. Although the original data set includes information on 61 different features about the articles, this report excludes and condenses some of those and focuses on the following 9 variables:  

```{r, echo=FALSE, results='asis'}
Name <- c("Channel", "Number_Title_Words", "Number_Content_Words", "Number_Images", "Number_Videos", "Positive_Word_Rate", "Negative_Word_Rate", "Title_Polarity", "Weekday", "Shares")
Definition <- c("Data channel is Lifestyle, Entertainment, Business, Social Media, Tech, or World", "Number of words in the title", "Number of words in the content", "Number of images", "Number of videos", "Rate of positive words in the content", "Rate of negative words in the content", "Title polarity", "Weekday published", "Number of shares")
vars <- cbind(Name, Definition)
knitr::kable(vars)
```

The purpose of this report is to look for patterns and to make predictions regarding the number of shares for articles in one of six different channels. Following some exploratory data analysis, four different models are used to model the response: a LASSO regression model, a forward stepwise regression model, a random forest model, and a boosted tree model.

# Load packages
```{r packages, message = FALSE}
library(readr)
library(dplyr)
library(knitr)
library(caret)
library(ggplot2)
library(GGally)
library(ggpubr)
```

# Data
## Read in and subset data
```{r subset, message = FALSE}
OnlineNewsPopularity <- read_csv("OnlineNewsPopularity.csv") 
OnlineNewsPopularity$url <- NULL

news <- OnlineNewsPopularity %>% 
  select("Number_Title_Words" = "n_tokens_title",
         "Number_Content_Words" = "n_tokens_content",
         "Number_Images" = "num_imgs",
         "Number_Videos" = "num_videos",
         starts_with("weekday_is"),
         starts_with("data_channel_is"),
         "Positive_Word_Rate" = "global_rate_positive_words",
         "Negative_Word_Rate" = "global_rate_negative_words",
         "Title_Polarity" = "title_sentiment_polarity",
         "Shares" = "shares")
  
news$Weekday <- as.factor(ifelse(news$weekday_is_monday == 1, "Monday",
                                 ifelse(news$weekday_is_tuesday == 1, "Tuesday", 
                                        ifelse(news$weekday_is_wednesday == 1, "Wednesday", 
                                               ifelse(news$weekday_is_thursday , "Thursday",
                                                      ifelse(news$weekday_is_friday == 1, "Friday",
                                                             ifelse(news$weekday_is_saturday == 1, "Saturday", "Sunday")))))))

news$Channel <- as.factor(ifelse(news$data_channel_is_lifestyle == 1, "Lifestyle",
                                 ifelse(news$data_channel_is_entertainment == 1, "Entertainment", 
                                        ifelse(news$data_channel_is_bus == 1, "Bus", 
                                               ifelse(news$data_channel_is_socmed , "Socmed",
                                                      ifelse(news$data_channel_is_tech == 1, "Tech", "World"))))))
news_final <- news %>%
  select(-c(starts_with("weekday_is"), starts_with("data_channel_is")))
```

## Automation
```{r automation}
news_data <- news_final %>% 
  filter(news_final$Channel == params$channel) %>% 
  select(-Channel)
```

## Split data into train and test
```{r split}
set.seed(216)
intrain <- createDataPartition(news_data$Shares, p = 0.7, list = FALSE)

training <- news_data[intrain,]
testing <- news_data[-intrain,]
```

# Summarization  
## Numeric summaries  
The following table displays five-number summaries for each of the numeric variables explored.
```{r summary}
stat <- training %>% 
  select(Number_Title_Words,
         Number_Content_Words,
         Number_Images,
         Number_Videos,
         Positive_Word_Rate,
         Negative_Word_Rate,
         Title_Polarity,
         Shares) %>% 
  apply(2, function(x){summary(x[!is.na(x)])}) 

kable(stat, caption = "Summary Stats for Numeric Variables", digits = 2)
```

## Pairs plot  
The following graphic displays the correlation between each of the variables explored. There are several things to look out for... The correlation between `Shares`, our response, and each of the other variables, our predictors. A higher value close to -1 or 1 indicates the two variables are highly correlated. A value close to 0 indicates little to no correlation. Additionally, one should consider correlation between two predictor variables as well. A high correlation between two predictor variables is an indication of collinearity, which should be taken into account when creating models later.
```{r ggpairs, message = FALSE}
training_sub <- training %>% 
  select(-Weekday)

ggpairs(training_sub)
```

## Barplot for weekday  
The following barplot displays counts for how many articles in a particular channel were published each day of the week over the time frame covered by the data set.
```{r barplot}
ggplot(training, aes(x = Weekday)) +
  geom_bar(fill = "medium blue", position = "dodge") +
  labs(y = "Count")
```

## Boxplot of weekday vs shares
```{r boxplot}
ggplot(training, aes(x = Weekday, y = Shares)) +
  geom_boxplot(color = "royal blue") +
  coord_flip() +
  scale_y_continuous(trans = "log10")
```

## Scatterplot of title length & polarity vs shares
```{r scatterplot}
ggplot(training, aes(x = Number_Title_Words, y = Shares)) + 
  geom_point(aes(color = Title_Polarity))
```

## Scatterplots of negative & positive word rate vs shares
```{r scatterplot2}
ggplot(training, aes(x = Positive_Word_Rate, y = Shares)) + 
  geom_point(size = 0.7, color = "royal blue") + 
  stat_cor(method = "pearson", label.x = 0, label.y = 100000, color = "royal blue") +
  xlim(0, 0.125) + ylim(0, 250000)

ggplot(training, aes(x = Negative_Word_Rate, y = Shares)) + 
  geom_point(size = 0.7, color = "dark blue") + 
  stat_cor(method = "pearson", label.x = 0, label.y = 100000, color = "dark blue") +
  xlim(0, 0.125) + ylim(0, 250000)
```

# Modeling
## Set up cross validation
```{r control}
control <- trainControl(method = "cv", number = 5)
```

## LASSO model
```{r lasso}
lasso_model <- train(Shares ~ .,
                     data = training,
                     method ='lasso',
                     preProcess = c("center", "scale"),
                     trControl = control)
predict(lasso_model$finalModel, type = "coef")
lasso_model$bestTune
```

## Forward stepwise model
```{r fwstep}
fwdstep_model <- train(Shares ~ .,
                       data = training,
                       method ='glmStepAIC',
                       preProcess = c("center", "scale"),
                       trControl = control,
                       direction = "forward",
                       trace = FALSE)
fwdstep_model
```

## Random forest model
```{r rf}
rf_model <- train(Shares ~ ., 
                  data = training, 
                  method = "rf", 
                  preProcess = c("center", "scale"), 
                  trControl = control, 
                  tuneGrid = expand.grid(mtry = 1:(ncol(training) - 1)))
rf_model
```
                         
## Boosted tree model
```{r gbm}
gbm_model <- train(Shares ~ .,
                   data = training,
                   method = "gbm",
                   trControl = control,
                   preProcess = c("center", "scale"),
                   verbose = FALSE)
gbm_model
```

# Comparison
## Apply model for prediction
```{r prediction}
lasso_predict <- predict(lasso_model, newdata = testing)
fwdstep_predict <- predict(fwdstep_model, newdata = testing)
rf_predict <- predict(rf_model, newdata = testing)
gbm_predict <- predict(gbm_model, newdata = testing)
```

## Model performance
```{r performance}
a <- postResample(lasso_predict, obs = testing$Shares)
b <- postResample(fwdstep_predict, obs = testing$Shares)
c <- postResample(rf_predict, obs = testing$Shares)
d <- postResample(gbm_predict, obs = testing$Shares)

table <- as_tibble(rbind(a, b, c, d))
Model <- c("Lasso", "Forward_Stepwise", "Random_Forest", "Boosted_Tree")
performance_table <- cbind(Model, table)
performance_table
```

### Best model by RMSE criteria
```{r rmse}
performance_table %>% slice_min(RMSE)
```

### Best model by Rsquared criteria
```{r rsquared}
performance_table %>% slice_max(Rsquared)
```
